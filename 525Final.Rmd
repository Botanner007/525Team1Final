---
title: "525Final"
author: "Team 1 - Jillian Chapman, Amanda Hall, Tariq Mansaray, Jennifer Snyder, David Tannerr"
date: "10/8/2019"
output: word_document
---

<<<<<<< HEAD
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
=======
```{r,include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
>>>>>>> 72b2d44d402a2e9683c81f12bbda885d75d63f01
```



##PROBLEM 3

In the Credit data in the ISLR package it contains 400 customers and information on their credit history. For full information of the data look at the help file. A company has approached us to better understand factors that influence the Balance variable which is average credit card balance in USD. Using the information in the model discuss the influential factors, and discuss the factors you choose to put in the model. Do you have any concerns about the use of certain variables in the model? Discuss how your model was created and any insights you can provide based on the results. HINT: Adding Gender and/or Ethnicity could be controversial or illegal in some uses of this this model you should discuss your decision on these variables and how it effects the organizations ability to use your model for prediction or inference.

To better understand factors that influence the average credit card balance variable ("Balance") we will explore various modeling methods and determine which variables provide us with the most useful inferences about the dataset. By determining variables to include and exclude in our analysis we will be able to provide the customer with some valuable insights for their business.

To begin, we evaluate our dataframe information.

```{r,include=FALSE}
library(dplyr)
library(ISLR)
library(alr4)
str(Credit)
head(Credit)
```

We begin by evaluating our dataframe information.  Our data, Credit, is derived from a simulated dataset containing information on 400 observations of 12 variables.  The summary shows that Balance could contain a zero value for some observations, but no other variables  contain zeros, N/As or negative values.  

```{r,include=TRUE}
summary(Credit)
```

<<<<<<< HEAD
Our dataset (Credit) is derived from a simulated dataset containing information on 400 observations on 12 variables.  The summary shows that Balance could contain a zero value for some observations. The dataset does not contain N/As or negative values in any of the variables. Credit providers would not be primarily interested in 0-balance customers, but instead would seek insights pertaining to customers that carry a balance on their account. In order to identify the significant relationships between Balance and the predictors, we will omit any observations that include 0 in the Balance variable. This subset of the Credit dataset, called Credit2, will be used for fitting models with a variety of methods, in order to determine the best fit, and draw some conclusions for the customer. 
=======
Since credit providers would primarily be interested in seeking insights pertaining to customers that carry a balance on their accounts, we will omit any observations that include zeros in the Balance variable. This new subset of the Credit data, called Credit2, will be used for fitting our models via a variety of methods, in order to determine the best fit and draw conclusions for our customer. 
>>>>>>> 72b2d44d402a2e9683c81f12bbda885d75d63f01

Problem Statement - What variables influence the Balance amount a customer retains? 

We first attempt to fit a model with no transformation of the variables, including zeros, to depict the influence of those 0s on the model.

```{r,include=FALSE}
CreditM1<-lm(Balance~., data=Credit)
summary(CreditM1)
<<<<<<< HEAD
=======
anova(CreditM1)
```

```{r,include=TRUE}
>>>>>>> 72b2d44d402a2e9683c81f12bbda885d75d63f01
par(mfrow=c(2,2))
plot(CreditM1)
anova(CreditM1)
```
<<<<<<< HEAD
While variance is fairly consistent, we can clearly see the Balnace=0 values skewing the plots. We will test some transformations to the Balance to see if a better model fit can be reached while including all of the observations (including Balance=0).
=======


While the variance shown in the plots is fairly consistent, we can clearly see the Balnace=0 values skewing the data.  We'll test some basic transformations (sqrt and ^2, but no log because of the zeros) on the Balance to see if a better model fit can be obtained while retaining all of the observations (including Balance=0).
>>>>>>> 72b2d44d402a2e9683c81f12bbda885d75d63f01


```{r,include=FALSE}
CreditM2<-lm(sqrt(Balance)~., data=Credit)
summary(CreditM2)
anova(CreditM2)

CreditM3<-lm(Balance^2~., data=Credit)
<<<<<<< HEAD
summary(CreditM4)
=======
summary(CreditM3)
anova(CreditM3)
```


```{r,include=TRUE}
par(mfrow=c(2,2))
plot(CreditM2)

par(mfrow=c(2,2))
plot(CreditM3)
```

The plots, shown after the transformations, are clearly still effected by the presence of the zeros. Thus, for the purpose of our analysis, omitting the zeros will allow for the most effective conclusions and will permit our models to provide clearer insight into the varibale relationships. We will continue to seek a well-fit model using the aforementioned filter and generate a subset, Credit2, which excludes the zeros. We can see in the summary data, the zeros in Balance have been removed.

```{r}
Credit2=Credit%>% filter(Balance>0)
plot(Credit2)
```

```{r,include=TRUE}
summary(Credit2)
```

...


```{r}
CR2Mod1<-lm(log(Balance)~., data=Credit2)
summary(CR2Mod1)
par(mfrow=c(2,2))
plot(CR2Mod1)
anova(CR2Mod1)

CR2Mod2<-lm(sqrt(Balance)~., data=Credit2)
summary(CR2Mod2)
par(mfrow=c(2,2))
plot(CR2Mod2)
anova(CR2Mod2)

CR2Mod3<-lm(Balance^2~., data=Credit2)
summary(CR2Mod3)
par(mfrow=c(2,2))
plot(CR2Mod3)
anova(CR2Mod3)

CR2Mod4<-lm(Balance~., data=Credit2)
summary(CR2Mod4)
par(mfrow=c(2,2))
plot(CR2Mod4)
anova(CR2Mod4)
```

CR2Mod4 has the highest R-squared value which means we can account for 99.94% of _________, but R-squared values shouldn't soley decide the fit of a model. If we review the plots generated, CR2Mod4 also appears to look most like we want to see.  In light of this information, we will continue with CR2Mod4 and eliminate any insignificant variables to create a simpler model.  Afterwards, we'll explore the effect of interactions in the hope that they would improve our model even more.

From the Credit2 subset's summary we can see that all the variables are significant at the 0.05 level except: ID, Education, Gender, Married and Ethnicity.  We will remove these variables and rerun our summary data.

```{r}
CR2Mod5<-lm(Balance~Income+Limit+Rating+Cards+Age+Student, data=Credit2)
summary(CR2Mod5)
par(mfrow=c(2,2))
plot(CR2Mod5)
anova(CR2Mod5)
anova(CR2Mod5,CR2Mod4)
```

Our ANOVA comparing CR2Mod4 (the bigger model) and CR2Mod5 (the smaller model) shows a p-value of greater than 0.05.  Due to this, we would reject the notion that the models are different and choose the simplier model to work with.  The review of our plots confirm the similarity of the the models.  To be sure the CR2Mod5 is exactly what we need, we'll run a ncvTest, boxCox and powerTransformation.

```{r}
ncvTest(CR2Mod5)
boxCox(CR2Mod5)
attach(Credit2)
summary(powerTransform(cbind(Balance,Income,Limit,Rating,Cards,Age,Student)))
detach(Credit2)
```

The ncvTest's p-value shows non-signficance; thus, constant variance is present.  The boxCox proves curious as there is no confidence level shown other than at 1 and the curve is not the typical arc expected. When we compare it to the powerTranform the likelihood ratios both show at significant level, confirming transforming to log or not transforming at all is not the same as using the recommended transformation provided.  The recommended transformation for all variables except the Student factor, appears to be to the power of 1.  Since increasing a variable to the power of one would simply be it's own number, we will opt to not transform the variables.  An move to add interactions in the hope that they will improve our model.

```{r}
CR2Mod6<-lm(Balance~Income+Limit+Rating+Cards+Age+Student+Income*Limit+Rating*Limit, data=Credit2)
summary(CR2Mod6)
par(mfrow=c(2,2))
plot(CR2Mod6)
anova(CR2Mod6)
anova(CR2Mod5,CR2Mod6)
```



```{r cahce=TRUE}


CR2Mod7=step(CR2Mod4,direction="backward")

CR2Intercept <- lm(Balance^(-1)~1, data=Credit2)
CR2Mod8 <- step(CR2Intercept, scope=list(lower=~1, upper=~ID+Income+Limit+Rating+Cards+Age+Education+Gender+Student+Married+Ethnicity), direction="forward", data=Credit2)

CR2Mod9<-step(CR2Intercept, scope=list(lower=~1, upper=~ID+Income+Limit+Rating+Cards+Age+Education+Gender+Student+Married+Ethnicity), data=Credit2)

```

The backwards step matches our chosen model, CR2Mod5.  While the forward step is different, it includes Ethnicity, a variable we previously determined, based on p-values, was not significant.  Thus, we have chosen to exclude the forward step.



### Problem 4


For this problem we used the Salaries data in the carData package to investigate the gender gap in the data. Our data represents the salaries of professors in the 2008-2009 year in an unspecified college. The data set consists of 397 observations with 6 variables each. The variables we will be looking at are "rank", "discipline", "yrs.since.PhD" or years since PhD, "yrs.service" or years of service, "sex", and finally "salary" as our response variable.

We looked at the help file and summary of the Salaries data to see exactly what we aer working with. 
```{r}
library(carData)
data("Salaries")
summary(Salaries)
```

Next, we plotted the data and viewed unique elements. 
```{r}
plot(Salaries)
str(Salaries)

```

We can see the data and the type of variables we are working with in the data. Each variable set seems to be correlated all over the place. Several have strage plots due to them being factor variables. 
```{r}
unique(Salaries$rank)
unique(Salaries$discipline)
unique(Salaries$sex)
```

We can also see the levels of each factors we are working with. 

Next, we want to see how many females are in each rank, and if this could play in role in salary difference between males and females for us to investigate further.
```{r}
summary(Salaries$rank)
table(Salaries$sex,Salaries$rank)
```

We can see that Female is pretty evenly distributed between the three ranks; however, Male is equally distirbuted between Assistant and Assocaite but is very heavily weighted in Full Professoor compared to the other two ranks. This could be because Male professors have more years of service than females, and therefore, males have had more time/opportunity to receive promotion. 

Next, we will look at the years of service for Male and Female to see if that can explain the heavier weight of Full Professors that are males. 
```{r}
table(Salaries$sex,Salaries$yrs.service)
```
We can see that there is only one female with more than 27 years of experience, with the majority of them below 20 years of expereince. 

Next, we are going to look at the preliminary difference in means.
```{r}
sexmeans <- tapply(Salaries$salary, Salaries$sex, mean)

sexmeans[2]-sexmeans[1]
```


Let's run a t-test now between the difference in means.
We ran a t-test to test that null hypothesis that the mean salary of males in this population is less than or equal to the mean salary of females, and the alternative hypothesis that the mean salary of males in this population is greater than the mean salary of females.
```{r}
t.test(x=Salaries$salary[Salaries$sex=="Male"], y=Salaries$salary[Salaries$sex=="Female"], alternative="greater")
isalariesfit <- lm(salary ~., data=Salaries)
```

Looking at the t-test, it looks like we are not able to accept the null at any significance level. We will go further and create models to help explain the salary data. 

We created a model to see what variables affect salary, with salary as the response variable and the remaining variables as the predictors. 
```{r}
salariesfit <- lm(salary ~., data=Salaries)
summary(salariesfit)
```
We can see that the intercept is 65955, which is the baseline salary in USD for a female, assistant professor with a discipline of 'a'.
I can see that all other variables held constant that the sex of Male would increase that baseline salary by 4783; however, looking at the p-value of 0.215, indicates it is not significant at the 0.05 or 0.01 level. Also, yrs.since.phd and yrs.service are not significant at the 0.01 level. Also, I can see how salary increases when faculty members get promoted to an Associate professor and to a Full professor.

Next, we used an ANOVA to compare the significance of each variable using their variance.
```{r}
anova(salariesfit)
```
The anova tells us that years since phd and sex seem to be variables that do not matter in this model based on their p-values. It could possibly be due to the fact that we only have data for 39 female professors, and 358 male professors. In general, we may not have enough data to show if there is truly a gender gap in pay. The more important question is is there a gender gap in hire from what our descriptive data tells us? Let's check for assumptions.

We plotted the model to check for non-constant, variance, normaility and outliers in the data, and we used ncvTest 
```{r}
par(mfrow=c(2,2))
plot(salariesfit)
```

```{r}
ncvTest(salariesfit)
```
We can see that normalization of residuals and the non constant variance assumptions have been violated. It is a possibility of a curvature in our model as well. Let's try transforming our response variable using the box-cox method.

We looked at the boxCox to see how and if we should transform the repsponse variable.
```{r}
boxCox(salariesfit)
```
Our lambda is -1, meaning to transform our resposne variable with a power of -1. 

Then, we took the inverse of our response variable. 

```{r}
salariesassump <- lm(salary^(-1)~rank+discipline+yrs.since.phd+yrs.service+sex, data=Salaries)
summary(salariesassump)
```
We can see that years since phd still seems to be insignificant at any level, and the adjustment for male is not significant when we test at a 5% significance level

```{r}
par(mfrow=c(2,2))
plot(salariesassump)
ncvTest(salariesassump)
```
As far as our assumptions, they are better than the previous model before our transformation. 

Next, we took a full transformation of all our variables.
```{r}
summary(powerTransform(cbind(salary, rank, discipline, yrs.since.phd, yrs.service, sex) ~1, Salaries, family="bcnPower"))
```
The lambda values suggest us to transform the variables "rank" and "sex", which are factor variables; however, using the power of a factor has absolutely no interpretability. The suggestion of putting yrs.since.phd and yrs.service also suggest to use the power of 0.5, which is the same as taking the square root of those variables. Interpretation may become an issue with the latter variables as well. 

```{r}
salariesassump2 <- lm(salary^(-1)~rank+discipline+I(yrs.since.phd^(0.5))+I(yrs.service^(0.5))+sex, data=Salaries)
summary(salariesassump2)

```
```{r}
par(mfrow=c(2,2))
plot(salariesassump2)
ncvTest(salariesassump2)
```

The assumptions are "more" met compared to the original model, but it seems as if the non constant variance is slightly worse compared to the second model. Also, the interpretability of using the power of 0.5, or the square root of predictor variables is very hard and hazy. 

Due to these factors, we decided to utilize the second model, where we only transformed the response variable. 

```{r}
anova(salariesassump)
```

Years since pHD is once again a variable that doesn't seem to matter in our model. Sex may be a usless variable as well if we test it against a significance of 5%. 

To fully test the best model we can use, we are finally going to implement stepwise selection using backwards, forwards, and the stepwise methods.
```{r}
mod <- step(salariesassump, scope=list(lower=~1, upper=~rank+discipline+yrs.since.phd+yrs.service+sex), data=Salaries)
```
The stepwise method says using all variables except yrs.since.phd is acceptable. 

Next, we used the backwards method. 
```{r}
mod2 <- step(salariesassump, scope=list(lower=~1, upper=~rank+discipline+yrs.since.phd+yrs.service+sex), direction="backward", data=Salaries)
```
The backwards method agrees to only remove yrs.since.phd. 

Finally, we used the forward method. 
```{r}
salariesintc. <- lm(salary^(-1)~1, data=Salaries)
mod3 <- step(salariesintc., scope=list(lower=~1, upper=~rank+discipline+yrs.since.phd+yrs.service+sex), direction="forward", data=Salaries)

```
All models seem to agree that the only variable that we shold remove is yrs.since.phD. 

We looked at the summary table of our linear regression model after the transformation of our response variable. 
```{r}
salariesassump3 <- lm(salary^(-1)~rank+discipline+yrs.service+sex, data=Salaries)
summary(salariesassump3)
```


Now to interpret the data, it looks as if the change betwen a female professor and a male is not significant, in other words the difference in salaries is negligible. Professors that are in theoretical departments seem to get paid more than those that work in applied departments. Of course, going from an assistant professor to a associate professor tends to see a jump in salary and likewise, going from associate professor to just professor sees another jump in one's salary. to put things into perspective, we can confidently say if you had came into a college as an assistant professor in a theoretical department you would more than likely have the lowest salary relatively speaking. As the years go by, your salary should also increase. Now in contrast, if you were to come into a university as a full Professor teaching in an applied department, you would likely see a higher salary.

From our data, we see that sex is an important variable, but it is difficult to identify if there is a gender gap between Male and Female. Looking at the summary table of our linear regression model after the transformation, the adjustment from female to male seems to not play significant role when we are testing at a 5% significance level; however, at a 10% level, there is a difference in pay between genders, which indicates males make slightly less when we adjust from female to male. However, our original t-test tells us that there is a gender gap in pay. There may be a discrimination in pay when it comes to gender, but with more males in our data than females in the data set that was presented to us, it is very hard to identify. Lastly, our anova model states that sex is a useless variable wherelese our stepwise regression opted to keep it. Just as was said before, the amount of females to males may be skewing the data. 















