---
title: "525Final"
author: "Team 1 - Jillian Chapman, Amanda Hall, Tariq Mansaray, Jennifer Snyder, David Tannerr"
date: "10/8/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





##PROBLEM 3

Question - To better understand factors that influence the Balance variable.  Discuss the influential factors and the factors you choose to put in the model.

To begin, we evaluate our dataframe information.

```{r}
library(dplyr)
library(ISLR)
library(alr4)
str(Credit)
head(Credit)
summary(Credit)
```

Our data is derived from a simulated data set containing information on ten thousand customers.  However, we have only been provided with 400 observations on the 12 variables.  We can see from the initial summary that Balance could contain a zero balance.  Since credit card companies typically make their money from the interest paid by a balance holder, we will subset the data to include only clients with a balance greater than zero.   There are no n/a's or negative data, so we will not have to consider these situations.

Problem Statement - What variables influence the amount of balance a client retains? 


```{r}
Credit2=Credit%>% filter(Balance>0)
summary(Credit2)
plot(Credit2)
```

Our initial model will contain all variables present in the subsetted data set as predictors, except Balance, which will be the response variable.

```{r}
CRMod1<-lm(Balance~., data=Credit2)
summary(CRMod1)
par(mfrow=c(2,2))
plot(CRMod1)
anova(CRMod1)
```

From the subset's summary we can see that all the variables are significant at the 0.05 level except: ID, Education, Gender, Married and Ethnicity.  We will remove these variables and rerun our summary data.

```{r}
CRMod2<-lm(Balance~Income+Limit+Rating+Cards+Age+Student, data=Credit2)
summary(CRMod2)
par(mfrow=c(2,2))
plot(CRMod2)
anova(CRMod2)
anova(CRMod2,CRMod1)
```

Our ANOVA comparing CRMod1 (the bigger model) and CRMod2 (the smaller model) shows a p-value of greater than 0.05.  Due to this, we would reject the notion that the models are different and choose the simplier model to work with.  The review of our plots confirm the similarity of the the models.  To be sure the CRMod2 is exactly what we need, we'll run a ncvTest, boxCox and powerTransformation.

```{r}
residualPlot(CRMod2)
ncvTest(CRMod2)
boxCox(CRMod2)
attach(Credit2)
summary(powerTransform(cbind(Balance,Income,Limit,Rating,Cards,Age,Student)))
detach(Credit2)
```

The ncvTest's p-value shows non-signficance; thus, constant variance is present.  The boxCox proves curious as there is no confidence level shown other than 1 and the curve is not the typical arc expected. When we compare it to the powerTranform the likelihood ratios both show at significant level, confirming transforming to log or not transforming at all is not the same as using the recommended transformation provided.  The recommended transformation for all variables except the Student factor, appears to be to the power of 1.  Since increasing a variable to the power of one would simply be it's own number, we will opt to not transform the variables.

```{r}
CRMod3<-lm(Balance~Income+Limit+Rating+Cards+Age+Student+Income*Limit+Rating*Limit, data=Credit2)
summary(CRMod4)
par(mfrow=c(2,2))
plot(CRMod3)
anova(CRMod3)
anova(CRMod2,CRMod3)
```






### Problem 4


For this problem we used the Salaries data in the carData package to investigate the gender gap in the data. 

We looked at the help file and summary of the Salaries data. 
```{r}
library(carData)
data("Salaries")
help(Salaries)
summary(Salaries)
```
Next, we plotted the data and viewed unique elements. 
```{r}
plot(Salaries)
str(Salaries)
unique(Salaries$rank)
unique(Salaries$discipline)
unique(Salaries$sex)
```
We can see the data and the type of variables we are working with in the data. 

```{r}
unique(Salaries$rank)
unique(Salaries$discipline)
unique(Salaries$sex)
```
We can also see the levels of each factors we are working with. 

Next, we want to see how many females are in each rank, and if this could play in role in salary difference between males and females for us to investigate further.
```{r}
summary(Salaries$rank)
table(Salaries$sex,Salaries$rank)
```
We can see that Female is pretty evenly distributed between the three ranks; however, Male is equally distirbuted between Assistant and Assocaite but is very heavily weighted in Full Professoor compared to the other two ranks. This could be because Male professors have more years of service than females, and therefore, males have had more time/opportunity to receive promotion. 

Next, we will look at the years of service for Male and Female to see if that can explain the heavier weight of Full Professors that are males. 
```{r}
table(Salaries$sex,Salaries$yrs.service)
```
We can see that there is only one female with more than 27 years of experience, with the majority of them below 20 years of expereince. 

Next, we are going to look at the preliminary difference in means.
```{r}
sexmeans <- tapply(Salaries$salary, Salaries$sex, mean)

sexmeans[2]-sexmeans[1]
```


Let's run a t-test now between the difference in means.
We ran a t-test to test that null hypothesis that the mean salary of males in this population is less than or equal to the mean salary of females, and the alternative hypothesis that the mean salary of males in this population is greater than the mean salary of females.
```{r}
t.test(x=Salaries$salary[Salaries$sex=="Male"], y=Salaries$salary[Salaries$sex=="Female"], alternative="greater")
isalariesfit <- lm(salary ~., data=Salaries)
```
Using a significance level of 0.01, I would accept the null hypothesis that the mean salary of males is less than or equal to the mean salary of females.

We created a model to see what variables affect salary, with salary as the response variable and the remaining variables as the predictors. 
```{r}
salariesfit <- lm(salary ~., data=Salaries)
summary(salariesfit)
```
We can see that the intercept is 65955, which is the baseline salary in USD for a female, assistant professor with a discipline of 'a'. ## Ph.D??##
I can see that all other variables held constant that the sex of Male would increase that baseline salary by 4783; however, looking at the p-value of 0.215, indicates it is not significant at the 0.05 or 0.01 level. Also, yrs.since.phd and yrs.service are not significant at the 0.01 level. Also, I can see how salary increases when faculty members get promoted to an Associate professor and to a Full professor.

Next, we used an ANOVA to compare the significance of the mean of the data.
## IS THAT CORRECT WORDING? ^^ ###
```{r}
anova(salariesfit)
```
The anova tells us that years since phd and sex seem to be variables that do not matter in this model based on their p-values. It could possibly be due to the fact that we only have data for 39 female professors, and 358 male professors. In general, we may not have enough data to show if there is truly a gender gap in pay. The more important question is a gender gap in hire from what our descriptive data tells us. Let's check for assumptions.

We plotted the model to check for non-constant, variance, normaility and outliers in the data, and we used ncvTest 
```{r}
par(mfrow=c(2,2))
plot(salariesfit)
```

```{r}
ncvTest(salariesfit)
```
We can see that normalization of residuals and the non constant variance assumptions have been violated. It is a possibility of a curvature in our model as well. Let's try transforming our response variable using the box-cox method.

We looked at the boxCox to see how and if we should transform the repsponse variable.
```{r}
boxCox(salariesfit)
```
Our lambda is -1, meaning to transform our variable predictor with a power of -1. 

Then, we took the inverse of our response variable. 
### TOOK THE INVERSE OF RESPONSE OR PREDICTOR? ##
```{r}
salariesassump <- lm(salary^(-1)~rank+discipline+yrs.since.phd+yrs.service+sex, data=Salaries)
summary(salariesassump)
```
We can see that years since phd still seems to be insignificant at any level, and the adjustment for male is not significant when we test at a 5% significance level

```{r}
par(mfrow=c(2,2))
plot(salariesassump)
ncvTest(salariesassump)
```
As far as our assumptions, they are better than the previous model before our transformation. 

Next, we took a full transformation of all our variables.
```{r}
summary(powerTransform(cbind(salary, rank, discipline, yrs.since.phd, yrs.service, sex) ~1, Salaries, family="bcnPower"))
```
The lambda values suggest us to transform the variables "rank" and "sex", which are factor variables; however, using the power of a factor has absolutely no interpretability. The suggestion of putting yrs.since.phd and yrs.service also suggest to use the power of 0.5, which is the same as taking the square root of those variables. Interpretation may become an issue with the latter variables as well. 

```{r}
salariesassump2 <- lm(salary^(-1)~rank+discipline+I(yrs.since.phd^(0.5))+I(yrs.service^(0.5))+sex, data=Salaries)
summary(salariesassump2)

```
```{r}
par(mfrow=c(2,2))
plot(salariesassump2)
ncvTest(salariesassump2)
```

The assumptions are "more" met compared to the original model, but it seems as if the non constant variance is slightly worse compared to the second model. Also, the interpretability of using the power of 0.5, or the square root of predictor variables is very hard and hazy. 

Due to these factors, we decided to utilize the second model, where we only transformed the response variable. 

```{r}
anova(salariesassump)
```

Years since pHD is once again a variable that doesn't seem to matter in our model. Sex may be a usless variable as well if we test it against a significance of 5%. 

To fully test the best model we can use, we are finally going to implement stepwise selection using backwards, forwards, and the stepwise methods.
```{r}
mod <- step(salariesassump, scope=list(lower=~1, upper=~rank+discipline+yrs.since.phd+yrs.service+sex), data=Salaries)
```
The stepwise method says using all variables except yrs.since.phd is acceptable. 

Next, we used the backwards method. 
```{r}
mod2 <- step(salariesassump, scope=list(lower=~1, upper=~rank+discipline+yrs.since.phd+yrs.service+sex), direction="backward", data=Salaries)
```
The backwards method agrees to only remove yrs.since.phd. 

Finally, we used the forward method. 
```{r}
salariesintc. <- lm(salary^(-1)~1, data=Salaries)
mod3 <- step(salariesintc., scope=list(lower=~1, upper=~rank+discipline+yrs.since.phd+yrs.service+sex), direction="forward", data=Salaries)

```
All models seem to agree that the only variable that we shold remove is yrs.since.phD. 

We looked at the summary table of our linear regression model after the transformation of our response variable. 
```{r}
salariesassump3 <- lm(salary^(-1)~rank+discipline+yrs.service+sex, data=Salaries)
summary(salariesassump3)
```

Now to interpret what our data is trying to tell us..........


From our data, we see that sex is an important variable, but it is difficult to identify if there is a gender gap between Male and Female. Looking at the summary table of our linear regression model after the transformation, the adjustment from female to male seems to not play significant role when we are testing at a 5% significance level; however, at a 10% level, there is a difference in pay between genders, which indicates males make slightly less when we adjust from female to male. However, our original t-test tells us that there is a gender gap in pay. There may be a discrimination in pay when it comes to gender, but with more males in our data than females in the data set that was presented to us, it is very hard to identify. 



